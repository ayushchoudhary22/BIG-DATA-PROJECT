{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# installs PySpark + common libs\n",
        "!pip install -q pyspark pandas pyarrow kafka-python\n",
        "print(\"Installed pyspark, pandas, pyarrow, kafka-python (if needed).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tALkEP6zNCgf",
        "outputId": "b497df68-143d-4d56-9052-1e52722a25d0"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installed pyspark, pandas, pyarrow, kafka-python (if needed).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Python cell\n",
        "!pip install -q pyspark pandas pyarrow\n",
        "print(\"Dependencies installed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkTrIABQQzi0",
        "outputId": "87315dd3-bbe1-4894-814d-16e1ea51cded"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependencies installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk-sHMJiLs_o",
        "outputId": "86593730-b4b8-4e48-c9e8-218e8b650b79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streaming job created: /content/smart_city_run/streaming_job_fixed.py\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "cat > /content/smart_city_run/streaming_job_fixed.py << 'PY'\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SmartCityStreamingFixed\").getOrCreate()\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "poll_schema = StructType([\n",
        "    StructField(\"timestamp\", LongType(), True),\n",
        "    StructField(\"location\", StringType(), True),\n",
        "    StructField(\"pm2_5\", DoubleType(), True),\n",
        "])\n",
        "\n",
        "traf_schema = StructType([\n",
        "    StructField(\"timestamp\", LongType(), True),\n",
        "    StructField(\"road\", StringType(), True),\n",
        "    StructField(\"vehicles\", IntegerType(), True),\n",
        "])\n",
        "\n",
        "poll = spark.readStream.format(\"csv\").option(\"header\",\"true\").schema(poll_schema).load(\"/content/smart_city_run/kafka_mock/pollution\")\n",
        "traf = spark.readStream.format(\"csv\").option(\"header\",\"true\").schema(traf_schema).load(\"/content/smart_city_run/kafka_mock/traffic\")\n",
        "\n",
        "def handle_poll(batch_df, batch_id):\n",
        "    print(\"\\n=== POLLUTION BATCH\", batch_id, \"===\")\n",
        "    if batch_df.rdd.isEmpty():\n",
        "        print(\"No new pollution data.\")\n",
        "        return\n",
        "    agg = batch_df.groupBy(\"location\").avg(\"pm2_5\").withColumnRenamed(\"avg(pm2_5)\",\"avg_pm2_5\")\n",
        "    agg.show(truncate=False)\n",
        "\n",
        "def handle_traf(batch_df, batch_id):\n",
        "    print(\"\\n=== TRAFFIC BATCH\", batch_id, \"===\")\n",
        "    if batch_df.rdd.isEmpty():\n",
        "        print(\"No new traffic data.\")\n",
        "        return\n",
        "    agg = batch_df.groupBy(\"road\").avg(\"vehicles\").withColumnRenamed(\"avg(vehicles)\",\"avg_vehicles\")\n",
        "    agg.show(truncate=False)\n",
        "\n",
        "p_query = poll.writeStream.foreachBatch(handle_poll).trigger(processingTime=\"3 seconds\").start()\n",
        "t_query = traf.writeStream.foreachBatch(handle_traf).trigger(processingTime=\"3 seconds\").start()\n",
        "\n",
        "print(\"Streaming job started (foreachBatch). Waiting for batches...\")\n",
        "p_query.awaitTermination()\n",
        "t_query.awaitTermination()\n",
        "PY\n",
        "\n",
        "echo \"Streaming job created: /content/smart_city_run/streaming_job_fixed.py\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "WORK=/content/smart_city_run\n",
        "\n",
        "cat > $WORK/pollution_producer_colab.py <<'PY'\n",
        "#!/usr/bin/env python3\n",
        "import time, csv, os, random, uuid\n",
        "OUT = os.path.join(os.path.dirname(__file__),\"kafka_mock/pollution\")\n",
        "os.makedirs(OUT, exist_ok=True)\n",
        "for i in range(40):\n",
        "    msg = {\"timestamp\":int(time.time()), \"location\":random.choice([\"locA\",\"locB\",\"locC\"]), \"pm2_5\":round(random.uniform(5,250),2)}\n",
        "    tmp = os.path.join(OUT, \"tmp_\"+uuid.uuid4().hex + \".csv\")\n",
        "    final = os.path.join(OUT, \"msg_\"+uuid.uuid4().hex + \".csv\")\n",
        "    with open(tmp,\"w\",newline=\"\") as f:\n",
        "        w=csv.DictWriter(f, fieldnames=[\"timestamp\",\"location\",\"pm2_5\"]); w.writeheader(); w.writerow(msg)\n",
        "    os.replace(tmp, final)\n",
        "    print(\"Wrote\", final)\n",
        "    time.sleep(0.5)\n",
        "PY\n",
        "\n",
        "cat > $WORK/traffic_producer_colab.py <<'PY'\n",
        "#!/usr/bin/env python3\n",
        "import time, csv, os, random, uuid\n",
        "OUT = os.path.join(os.path.dirname(__file__),\"kafka_mock/traffic\")\n",
        "os.makedirs(OUT, exist_ok=True)\n",
        "for i in range(40):\n",
        "    msg = {\"timestamp\":int(time.time()), \"road\":random.choice([\"R1\",\"R2\",\"R3\"]), \"vehicles\":random.randint(0,200)}\n",
        "    tmp = os.path.join(OUT, \"tmp_\"+uuid.uuid4().hex + \".csv\")\n",
        "    final = os.path.join(OUT, \"msg_\"+uuid.uuid4().hex + \".csv\")\n",
        "    with open(tmp,\"w\",newline=\"\") as f:\n",
        "        w=csv.DictWriter(f, fieldnames=[\"timestamp\",\"road\",\"vehicles\"]); w.writeheader(); w.writerow(msg)\n",
        "    os.replace(tmp, final)\n",
        "    print(\"Wrote\", final)\n",
        "    time.sleep(0.5)\n",
        "PY\n",
        "\n",
        "echo \"Producer scripts created at $WORK/\"\n",
        "ls -la /content/smart_city_run | sed -n '1,200p'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4BK_oVRMoel",
        "outputId": "c3028247-1ed9-459f-acba-f554546de9c0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Producer scripts created at /content/smart_city_run/\n",
            "total 44\n",
            "drwxr-xr-x 3 root root 4096 Nov 21 16:54 .\n",
            "drwxr-xr-x 1 root root 4096 Nov 21 16:29 ..\n",
            "drwxr-xr-x 4 root root 4096 Nov 21 16:29 kafka_mock\n",
            "-rw-r--r-- 1 root root  656 Nov 21 16:54 pollution_producer_colab.py\n",
            "-rw-r--r-- 1 root root  824 Nov 21 16:31 pollution_producer_mock.py\n",
            "-rw-r--r-- 1 root root 2833 Nov 21 16:44 streaming_job_colab.py\n",
            "-rw-r--r-- 1 root root 1308 Nov 21 16:31 streaming_job_filesource.py\n",
            "-rw-r--r-- 1 root root 1763 Nov 21 16:54 streaming_job_fixed.py\n",
            "-rw-r--r-- 1 root root  637 Nov 21 16:54 traffic_producer_colab.py\n",
            "-rw-r--r-- 1 root root  803 Nov 21 16:31 traffic_producer_mock.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Python cell\n",
        "!python3 /content/smart_city_run/streaming_job_colab.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewICzAvINZcV",
        "outputId": "db513516-59a4-4dfb-ac5a-e34abcb98109"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/11/21 16:55:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/11/21 16:55:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/11/21 16:55:05 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
            "No JSON stream_input files found; continuing to watch producer CSV folders.\n",
            "25/11/21 16:55:11 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d966ece1-d545-4de3-9e37-82906fa4fee7. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
            "25/11/21 16:55:11 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
            "25/11/21 16:55:12 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-97f31a0c-9faf-4315-9e1a-be2da1128e1f. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
            "25/11/21 16:55:12 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
            "Streaming (filesource) started. Watching /content/smart_city_run/kafka_mock/* and stream_input (if present).\n",
            "-------------------------------------------\n",
            "Batch: 0\n",
            "-------------------------------------------\n",
            "+----+-----------------+\n",
            "|road|avg_vehicles     |\n",
            "+----+-----------------+\n",
            "|R3  |96.33333333333333|\n",
            "|R2  |105.1875         |\n",
            "|R1  |93.55555555555556|\n",
            "+----+-----------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 0\n",
            "-------------------------------------------\n",
            "+--------+------------------+\n",
            "|location|avg_pm2_5         |\n",
            "+--------+------------------+\n",
            "|locC    |112.44200000000001|\n",
            "|locB    |126.22            |\n",
            "|locA    |104.066875        |\n",
            "+--------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/smart_city_run/pollution_producer_colab.py\n"
      ],
      "metadata": {
        "id": "yjUZXNo6MgaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/smart_city_run/traffic_producer_colab.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XzcRTQPNxQh",
        "outputId": "4d5014ea-7ead-4b06-9437-b98d501de08a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_dc3f038ef1444bf5b0af1a3ce7b126a8.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_dd0f8072dc294a3a903214c5b2cd066a.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_cc7665d7df0247d2b4884735efa76daf.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_9db6b6fe497841e8893110dd334db959.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_4f1c7ca5960341508b7a10451ad082fc.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_476ad2f42a53445e991b4e5c9735a663.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_d02865a0c7814687b00383853e031392.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_bb2a6f12e3da45fea9240da736703b1c.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_5afa4cb04ffe4bf3849c851503461ecd.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_eb81df53130e4475a0051e700140794a.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_d28334c538cc4efabd64860b1e43cb06.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_2a26817929a34dabb12f84ff195111fd.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_35133c28141647ec9c1c09f523168d3b.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_11dec333b7cc4df4a91a3bc89f72b0db.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_1b74f7d362ba44d78d6d30fee2bb1122.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_c608950fca94427eb23e7f9dc13d52d6.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_260d9f98bae74a4bbc5cfdcaf7dc26b8.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_e5673a6cf13f44ddb42594b8e96df294.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_a7424b8fa32343368b3cd680d470df9b.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_9d2b7bf1755a4296a55368626e0ff902.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_8e17772b71874fd98a89db8c00243aaa.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_e066ab84afb941008804185494a5554e.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_9e2aef7681194ac8b2eacf4cc2742d02.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_5a84ab14c88d4cc49f91a8f00fcb754e.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_d767b689913540608c0f0b663126cad8.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_9dc5bff22142489ba81f515a74dc65bf.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_38719c2a1c1c4a73b08167a5f0998609.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_aad43c4ec2af4c1faba43a62b9126869.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_096979f2bd5a44b8a6ca3cf0fb93d47b.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_fc71d730ff2245fcaf0597af3bf0bb6d.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_27bfbfaeba9d4bc3866e8d4e8c291720.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_bddadee4de684c4e96cb2abcd8eb7a0b.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_3df6b9401ebe4e5697836a4bb4d18bd8.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_5eecea81551941b99ac5d5df396fa818.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_05716e8bb4f24a7286db2df3002f2acb.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_a92b7586dc974439a5fab926549c64b7.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_2f7922baf2d346a7ba5b1cb6a4c33013.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_f4818a11382c4e8e82499a2b4b29ec51.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_76592696c0bf4903afdca2db57332a15.csv\n",
            "Wrote /content/smart_city_run/kafka_mock/traffic/msg_a12221480bac48b5bbf03df488ff1a76.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this in a Python cell\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, LongType, StringType, DoubleType, IntegerType\n",
        "spark = SparkSession.builder.appName(\"BatchCheck\").getOrCreate()\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "poll_schema = StructType([\n",
        "    StructField(\"timestamp\", LongType(), True),\n",
        "    StructField(\"location\", StringType(), True),\n",
        "    StructField(\"pm2_5\", DoubleType(), True),\n",
        "])\n",
        "traf_schema = StructType([\n",
        "    StructField(\"timestamp\", LongType(), True),\n",
        "    StructField(\"road\", StringType(), True),\n",
        "    StructField(\"vehicles\", IntegerType(), True),\n",
        "])\n",
        "\n",
        "poll_df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(poll_schema).load(\"/content/smart_city_run/kafka_mock/pollution\")\n",
        "traf_df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(traf_schema).load(\"/content/smart_city_run/kafka_mock/traffic\")\n",
        "\n",
        "print(\"Pollution count:\", poll_df.count())\n",
        "print(\"Traffic count:\", traf_df.count())\n",
        "\n",
        "print(\"Pollution aggregation:\")\n",
        "poll_df.groupBy(\"location\").avg(\"pm2_5\").withColumnRenamed(\"avg(pm2_5)\",\"avg_pm2_5\").show(truncate=False)\n",
        "\n",
        "print(\"Traffic aggregation:\")\n",
        "traf_df.groupBy(\"road\").avg(\"vehicles\").withColumnRenamed(\"avg(vehicles)\",\"avg_vehicles\").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TCdvkZRU1Mf",
        "outputId": "b56194f2-095a-4753-8cc7-ad5886c9d84b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pollution count: 40\n",
            "Traffic count: 40\n",
            "Pollution aggregation:\n",
            "+--------+------------------+\n",
            "|location|avg_pm2_5         |\n",
            "+--------+------------------+\n",
            "|locC    |112.44199999999998|\n",
            "|locB    |126.22            |\n",
            "|locA    |104.06687500000001|\n",
            "+--------+------------------+\n",
            "\n",
            "Traffic aggregation:\n",
            "+----+-----------------+\n",
            "|road|avg_vehicles     |\n",
            "+----+-----------------+\n",
            "|R3  |96.33333333333333|\n",
            "|R2  |105.1875         |\n",
            "|R1  |93.55555555555556|\n",
            "+----+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q dash plotly pandas pyarrow fastparquet pyngrok\n",
        "print(\"Installed dash, plotly, pandas, pyngrok ...\")\n"
      ],
      "metadata": {
        "id": "8K1HQ490VVty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIONAL: set your ngrok auth token (recommended to avoid rate limits)\n",
        "NGROK_AUTH_TOKEN = \"35lsQR8RxpBHi6QzRStxL5C6lvr_AKiwW9i7j2YVqsQhh4Nm\"   # <--- replace with your token or leave as \"\"\n",
        "from pyngrok import ngrok\n",
        "if NGROK_AUTH_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "    print(\"ngrok auth token set.\")\n",
        "else:\n",
        "    print(\"No ngrok token set; a temporary tunnel will be created.\")\n"
      ],
      "metadata": {
        "id": "fuL84R40WP1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Dash app file\n",
        "app_code = r'''\n",
        "from dash import Dash, dcc, html, Input, Output\n",
        "import pandas as pd, plotly.express as px, glob, os, time\n",
        "\n",
        "app = Dash(__name__)\n",
        "\n",
        "app.layout = html.Div([\n",
        "    html.H3(\"Smart City â€” Live Stream Dashboard\"),\n",
        "    dcc.Interval(id=\"interval\", interval=4000, n_intervals=0),\n",
        "    dcc.Graph(id=\"pm25-chart\"),\n",
        "    dcc.Graph(id=\"traffic-chart\"),\n",
        "    html.Div(id=\"last-update\")\n",
        "])\n",
        "\n",
        "def load_pollution():\n",
        "    path = \"/content/smart_city_run/kafka_mock/pollution\"\n",
        "    files = sorted(glob.glob(os.path.join(path,\"*.csv\")))\n",
        "    if not files:\n",
        "        return pd.DataFrame(columns=[\"timestamp\",\"location\",\"pm2_5\"])\n",
        "    df = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
        "    return df\n",
        "\n",
        "def load_traffic():\n",
        "    path = \"/content/smart_city_run/kafka_mock/traffic\"\n",
        "    files = sorted(glob.glob(os.path.join(path,\"*.csv\")))\n",
        "    if not files:\n",
        "        return pd.DataFrame(columns=[\"timestamp\",\"road\",\"vehicles\"])\n",
        "    df = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
        "    return df\n",
        "\n",
        "@app.callback(\n",
        "    Output(\"pm25-chart\",\"figure\"),\n",
        "    Output(\"traffic-chart\",\"figure\"),\n",
        "    Output(\"last-update\",\"children\"),\n",
        "    Input(\"interval\",\"n_intervals\")\n",
        ")\n",
        "def update(n):\n",
        "    p = load_pollution()\n",
        "    t = load_traffic()\n",
        "    if p.empty:\n",
        "        fig1 = px.bar(title=\"No pollution data yet\")\n",
        "    else:\n",
        "        agg = p.groupby(\"location\", as_index=False)[\"pm2_5\"].mean()\n",
        "        fig1 = px.bar(agg, x=\"location\", y=\"pm2_5\", title=\"Average PM2.5 by Location\")\n",
        "    if t.empty:\n",
        "        fig2 = px.bar(title=\"No traffic data yet\")\n",
        "    else:\n",
        "        agg2 = t.groupby(\"road\", as_index=False)[\"vehicles\"].mean()\n",
        "        fig2 = px.bar(agg2, x=\"road\", y=\"vehicles\", title=\"Average Vehicles by Road\")\n",
        "    return fig1, fig2, f\"Last update: {time.ctime()}\"\n",
        "\n",
        "# Expose the app object so the runner thread can import it\n",
        "app.server  # ensure server exists\n",
        "with open(\"/content/smart_city_dash.py\", \"w\") as f:\n",
        "    f.write(app_code)\n",
        "print(\"Dash app written to /content/smart_city_dash.py\")\n",
        "'''\n",
        "print(\"Writing Dash app...\")\n",
        "exec(app_code, {})\n"
      ],
      "metadata": {
        "id": "UNqfJizIVdX6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}